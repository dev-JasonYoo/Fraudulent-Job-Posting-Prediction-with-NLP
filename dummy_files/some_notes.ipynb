{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c1e795c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>location</th>\n",
       "      <th>description</th>\n",
       "      <th>requirements</th>\n",
       "      <th>telecommuting</th>\n",
       "      <th>has_company_logo</th>\n",
       "      <th>has_questions</th>\n",
       "      <th>fraudulent</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Marketing Intern</td>\n",
       "      <td>US, NY, New York</td>\n",
       "      <td>Food52, a fast-growing, James Beard Award-winn...</td>\n",
       "      <td>Experience with content management systems a m...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Customer Service - Cloud Video Production</td>\n",
       "      <td>NZ, , Auckland</td>\n",
       "      <td>Organised - Focused - Vibrant - Awesome!Do you...</td>\n",
       "      <td>What we expect from you:Your key responsibilit...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Commissioning Machinery Assistant (CMA)</td>\n",
       "      <td>US, IA, Wever</td>\n",
       "      <td>Our client, located in Houston, is actively se...</td>\n",
       "      <td>Implement pre-commissioning and commissioning ...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Account Executive - Washington DC</td>\n",
       "      <td>US, DC, Washington</td>\n",
       "      <td>THE COMPANY: ESRI – Environmental Systems Rese...</td>\n",
       "      <td>EDUCATION: Bachelor’s or Master’s in GIS, busi...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Bill Review Manager</td>\n",
       "      <td>US, FL, Fort Worth</td>\n",
       "      <td>JOB TITLE: Itemization Review ManagerLOCATION:...</td>\n",
       "      <td>QUALIFICATIONS:RN license in the State of Texa...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       title            location  \\\n",
       "0                           Marketing Intern    US, NY, New York   \n",
       "1  Customer Service - Cloud Video Production      NZ, , Auckland   \n",
       "2    Commissioning Machinery Assistant (CMA)       US, IA, Wever   \n",
       "3          Account Executive - Washington DC  US, DC, Washington   \n",
       "4                        Bill Review Manager  US, FL, Fort Worth   \n",
       "\n",
       "                                         description  \\\n",
       "0  Food52, a fast-growing, James Beard Award-winn...   \n",
       "1  Organised - Focused - Vibrant - Awesome!Do you...   \n",
       "2  Our client, located in Houston, is actively se...   \n",
       "3  THE COMPANY: ESRI – Environmental Systems Rese...   \n",
       "4  JOB TITLE: Itemization Review ManagerLOCATION:...   \n",
       "\n",
       "                                        requirements  telecommuting  \\\n",
       "0  Experience with content management systems a m...              0   \n",
       "1  What we expect from you:Your key responsibilit...              0   \n",
       "2  Implement pre-commissioning and commissioning ...              0   \n",
       "3  EDUCATION: Bachelor’s or Master’s in GIS, busi...              0   \n",
       "4  QUALIFICATIONS:RN license in the State of Texa...              0   \n",
       "\n",
       "   has_company_logo  has_questions  fraudulent  \n",
       "0                 1              0           0  \n",
       "1                 1              0           0  \n",
       "2                 1              0           0  \n",
       "3                 1              0           0  \n",
       "4                 1              1           0  "
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df1 = pd.read_csv(\"fake_job_postings.csv\")\n",
    "df1.drop(['benefits', 'company_profile', 'employment_type', 'salary_range',\n",
    "          'industry', 'department', 'required_experience', 'required_education', 'job_id', 'function',], axis=1, inplace=True)\n",
    "df2 = pd.read_csv(\"job_train.csv\")\n",
    "df2 = df2[df2['fraudulent']==1]\n",
    "merged_df = pd.concat([df1, df2])\n",
    "# merged_df.to_csv(\"final_data.csv\")\n",
    "merged_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "a077f822",
   "metadata": {},
   "outputs": [],
   "source": [
    "import unicodedata\n",
    "\n",
    "def remove_accents(text):\n",
    "    if pd.isna(text):\n",
    "        return text\n",
    "    # Normalize to NFD (decompose accented chars), then filter out combining marks\n",
    "    return ''.join(c for c in unicodedata.normalize('NFD', str(text))\n",
    "                   if unicodedata.category(c) != 'Mn')\n",
    "\n",
    "\n",
    "def clean_accents(column, print_change):\n",
    "    cleaned_lst = []\n",
    "    for loc in merged_df[column]:\n",
    "        if not pd.isna(loc) and str(loc).strip() != \"\": \n",
    "            cleaned_loc = remove_accents(loc)\n",
    "            if loc != cleaned_loc:\n",
    "                cleaned_lst.append((loc, cleaned_loc))\n",
    "    cleaned_lst = list(set(cleaned_lst))\n",
    "    if print_change:\n",
    "        for cleaned in cleaned_lst:\n",
    "            print(cleaned)\n",
    "    \n",
    "\n",
    "text_cols = ['title', 'location', 'description', 'requirements']\n",
    "for col in text_cols:\n",
    "    clean_accents(col, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9a8d2591",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique Unicode Characters Found:\n",
      "------------------------------------------------------------\n",
      "' ' | U+00A0 | NO-BREAK SPACE\n",
      "'Ø' | U+00D8 | LATIN CAPITAL LETTER O WITH STROKE\n",
      "'á' | U+00E1 | LATIN SMALL LETTER A WITH ACUTE\n",
      "'ã' | U+00E3 | LATIN SMALL LETTER A WITH TILDE\n",
      "'ä' | U+00E4 | LATIN SMALL LETTER A WITH DIAERESIS\n",
      "'é' | U+00E9 | LATIN SMALL LETTER E WITH ACUTE\n",
      "'ó' | U+00F3 | LATIN SMALL LETTER O WITH ACUTE\n",
      "'ö' | U+00F6 | LATIN SMALL LETTER O WITH DIAERESIS\n",
      "'ø' | U+00F8 | LATIN SMALL LETTER O WITH STROKE\n",
      "'ü' | U+00FC | LATIN SMALL LETTER U WITH DIAERESIS\n",
      "'İ' | U+0130 | LATIN CAPITAL LETTER I WITH DOT ABOVE\n",
      "'ł' | U+0142 | LATIN SMALL LETTER L WITH STROKE\n",
      "'ń' | U+0144 | LATIN SMALL LETTER N WITH ACUTE\n",
      "'Α' | U+0391 | GREEK CAPITAL LETTER ALPHA\n",
      "'Ε' | U+0395 | GREEK CAPITAL LETTER EPSILON\n",
      "'Η' | U+0397 | GREEK CAPITAL LETTER ETA\n",
      "'Ι' | U+0399 | GREEK CAPITAL LETTER IOTA\n",
      "'Κ' | U+039A | GREEK CAPITAL LETTER KAPPA\n",
      "'Λ' | U+039B | GREEK CAPITAL LETTER LAMDA\n",
      "'Ν' | U+039D | GREEK CAPITAL LETTER NU\n",
      "'Ο' | U+039F | GREEK CAPITAL LETTER OMICRON\n",
      "'ή' | U+03AE | GREEK SMALL LETTER ETA WITH TONOS\n",
      "'α' | U+03B1 | GREEK SMALL LETTER ALPHA\n",
      "'θ' | U+03B8 | GREEK SMALL LETTER THETA\n",
      "'ν' | U+03BD | GREEK SMALL LETTER NU\n",
      "'교' | U+AD50 | HANGUL SYLLABLE GYO\n",
      "'구' | U+AD6C | HANGUL SYLLABLE GU\n",
      "'동' | U+B3D9 | HANGUL SYLLABLE DONG\n",
      "'마' | U+B9C8 | HANGUL SYLLABLE MA\n",
      "'포' | U+D3EC | HANGUL SYLLABLE PO\n"
     ]
    }
   ],
   "source": [
    "# Assuming your dataframe is called 'df' and column is 'text_column'\n",
    "def find_unicode_chars(text_series):\n",
    "    \"\"\"Find all unique non-ASCII unicode characters in a text series\"\"\"\n",
    "    unicode_chars = set()\n",
    "    \n",
    "    for text in text_series.dropna():\n",
    "        for char in str(text):\n",
    "            # Check if character is non-ASCII (ord > 127)\n",
    "            if ord(char) > 127:\n",
    "                unicode_chars.add(char)\n",
    "    \n",
    "    return sorted(unicode_chars, key=lambda x: ord(x))\n",
    "\n",
    "# Get unique unicode characters\n",
    "unicode_list = find_unicode_chars(merged_df['description_and_requirements'])\n",
    "unicode_list = find_unicode_chars(merged_df['location'])\n",
    "\n",
    "# Print them with their unicode codes and names\n",
    "import unicodedata\n",
    "\n",
    "print(\"Unique Unicode Characters Found:\")\n",
    "print(\"-\" * 60)\n",
    "for char in unicode_list:\n",
    "    try:\n",
    "        name = unicodedata.name(char)\n",
    "    except ValueError:\n",
    "        name = \"Unknown\"\n",
    "    print(f\"'{char}' | U+{ord(char):04X} | {name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6060cd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option 1: Replace specific characters with their text equivalents\n",
    "replacements = {\n",
    "    '—': '--',        # em dash\n",
    "    '–': '-',         # en dash\n",
    "    '\"': '\"',         # smart quotes\n",
    "    '\"': '\"',\n",
    "    ''': \"'\",\n",
    "    ''': \"'\",\n",
    "    '…': '...',       # ellipsis\n",
    "    '®': '(R)',\n",
    "    '©': '(C)',\n",
    "    '™': '(TM)',\n",
    "}\n",
    "\n",
    "df['text_column'] = df['text_column'].str.replace('|'.join(replacements.keys()), \n",
    "                                                    lambda m: replacements[m.group()], \n",
    "                                                    regex=True)\n",
    "\n",
    "# Option 2: Remove accents from letters (keeping base letters)\n",
    "import unicodedata\n",
    "\n",
    "def remove_accents(text):\n",
    "    if pd.isna(text):\n",
    "        return text\n",
    "    # Normalize to NFD (decompose accented chars), then filter out combining marks\n",
    "    return ''.join(c for c in unicodedata.normalize('NFD', str(text))\n",
    "                   if unicodedata.category(c) != 'Mn')\n",
    "\n",
    "df['text_column'] = df['text_column'].apply(remove_accents)\n",
    "\n",
    "# Option 3: Convert to closest ASCII equivalent\n",
    "df['text_column'] = df['text_column'].str.encode('ascii', errors='ignore').str.decode('ascii')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2935bcad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "397\n",
      "10\n",
      "852\n",
      "446\n",
      "456\n"
     ]
    }
   ],
   "source": [
    "# print(merged_df.isnull().sum())\n",
    "print(df1.duplicated().sum())\n",
    "print(df2.duplicated().sum())\n",
    "print(merged_df.duplicated().sum())\n",
    "\n",
    "df1_unique = df1.drop_duplicates()\n",
    "df2_unique = df2.drop_duplicates()\n",
    "overlap_count = pd.merge(df1_unique, df2_unique, how='inner').shape[0]\n",
    "print(overlap_count)\n",
    "print(len(df2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "923c3da9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 18334 entries, 0 to 8938\n",
      "Data columns (total 14 columns):\n",
      " #   Column                        Non-Null Count  Dtype \n",
      "---  ------                        --------------  ----- \n",
      " 0   title                         18334 non-null  object\n",
      " 1   location                      17980 non-null  object\n",
      " 2   description                   18334 non-null  object\n",
      " 3   requirements                  15564 non-null  object\n",
      " 4   telecommuting                 18334 non-null  int64 \n",
      " 5   has_company_logo              18334 non-null  int64 \n",
      " 6   has_questions                 18334 non-null  int64 \n",
      " 7   fraudulent                    18334 non-null  int64 \n",
      " 8   has_requirements              18334 non-null  bool  \n",
      " 9   description_and_requirements  18334 non-null  object\n",
      " 10  country_state                 15735 non-null  object\n",
      " 11  country                       17980 non-null  object\n",
      " 12  has_location                  18334 non-null  bool  \n",
      " 13  has_location_details          18334 non-null  bool  \n",
      "dtypes: bool(3), int64(4), object(7)\n",
      "memory usage: 1.7+ MB\n",
      "title                           11231\n",
      "location                         3105\n",
      "description                     14801\n",
      "requirements                    11967\n",
      "telecommuting                       2\n",
      "has_company_logo                    2\n",
      "has_questions                       2\n",
      "fraudulent                          2\n",
      "has_requirements                    2\n",
      "description_and_requirements    15292\n",
      "country_state                     381\n",
      "country                            91\n",
      "has_location                        2\n",
      "has_location_details                2\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "merged_df.info()\n",
    "print(merged_df.nunique())\n",
    "merged_df.duplicated().sum()\n",
    "merged_df.drop_duplicates(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6c97bea9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "title                              0\n",
       "location                         341\n",
       "description                        0\n",
       "requirements                    2637\n",
       "telecommuting                      0\n",
       "has_company_logo                   0\n",
       "has_questions                      0\n",
       "fraudulent                         0\n",
       "has_requirements                   0\n",
       "description_and_requirements       0\n",
       "country_state                   2509\n",
       "country                          341\n",
       "has_location                       0\n",
       "has_location_details               0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "textual = ['title', 'company_profile', 'description',\n",
    "       'requirements', 'benefits']\n",
    "categorical = ['location', 'employment_type', 'industry']\n",
    "continuous = ['salary_range']\n",
    "label = ['fraudulent']\n",
    "\n",
    "merged_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "972e9cd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(merged_df.loc[merged_df['location_has_non_ascii'] == 1, 'location'].drop_duplicates())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "61072e30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      1.00      0.98      3336\n",
      "           1       0.98      0.30      0.46       161\n",
      "\n",
      "    accuracy                           0.97      3497\n",
      "   macro avg       0.97      0.65      0.72      3497\n",
      "weighted avg       0.97      0.97      0.96      3497\n",
      "\n",
      "[[ 0.67765924  1.34207408 -0.17258051 ... -0.18184741 -0.0522385\n",
      "  -0.0719484 ]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Combine relevant text columns (you can modify this based on which columns you want to use)\n",
    "merged_df['text'] = merged_df['title'].fillna('') + ' ' + merged_df['description'].fillna('')\n",
    "\n",
    "# Split the data\n",
    "X = merged_df['text']\n",
    "y = merged_df['fraudulent']\n",
    "\n",
    "# Split into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create and fit TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer(max_features=5000, stop_words='english')\n",
    "X_train_vec = vectorizer.fit_transform(X_train)\n",
    "X_test_vec = vectorizer.transform(X_test)\n",
    "\n",
    "# Train logistic regression model\n",
    "model = LogisticRegression(random_state=42, max_iter=1000)\n",
    "model.fit(X_train_vec, y_train)\n",
    "\n",
    "# Make predictions and print classification report\n",
    "y_pred = model.predict(X_test_vec)\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(model.coef_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "713615fd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "90d34fd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# len(df3)\n",
    "# len(merged_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4b16e48f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      1.00      0.98      3321\n",
      "           1       0.95      0.35      0.51       176\n",
      "\n",
      "    accuracy                           0.97      3497\n",
      "   macro avg       0.96      0.67      0.75      3497\n",
      "weighted avg       0.97      0.97      0.96      3497\n",
      "\n",
      "[[ 0.28998688  1.26500827 -0.21935576 ... -0.17891268 -0.05137752\n",
      "  -0.06817463]]\n"
     ]
    }
   ],
   "source": [
    "merged_df.drop_duplicates(inplace=True)\n",
    "# Combine relevant text columns (you can modify this based on which columns you want to use)\n",
    "merged_df['text'] = merged_df['title'].fillna('') + ' ' + merged_df['description'].fillna('')\n",
    "\n",
    "# Split the data\n",
    "X = merged_df['text']\n",
    "y = merged_df['fraudulent']\n",
    "\n",
    "# Split into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create and fit TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer(max_features=5000, stop_words='english')\n",
    "X_train_vec = vectorizer.fit_transform(X_train)\n",
    "X_test_vec = vectorizer.transform(X_test)\n",
    "\n",
    "# Train logistic regression model\n",
    "model = LogisticRegression(random_state=42, max_iter=1000)\n",
    "model.fit(X_train_vec, y_train)\n",
    "\n",
    "# Make predictions and print classification report\n",
    "y_pred = model.predict(X_test_vec)\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(model.coef_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9a366b98",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.int64(0)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_df.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d4323a23",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>location</th>\n",
       "      <th>description</th>\n",
       "      <th>requirements</th>\n",
       "      <th>telecommuting</th>\n",
       "      <th>has_company_logo</th>\n",
       "      <th>has_questions</th>\n",
       "      <th>fraudulent</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Marketing Intern</td>\n",
       "      <td>US, NY, New York</td>\n",
       "      <td>Food52, a fast-growing, James Beard Award-winn...</td>\n",
       "      <td>Experience with content management systems a m...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Customer Service - Cloud Video Production</td>\n",
       "      <td>NZ, , Auckland</td>\n",
       "      <td>Organised - Focused - Vibrant - Awesome!Do you...</td>\n",
       "      <td>What we expect from you:Your key responsibilit...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Commissioning Machinery Assistant (CMA)</td>\n",
       "      <td>US, IA, Wever</td>\n",
       "      <td>Our client, located in Houston, is actively se...</td>\n",
       "      <td>Implement pre-commissioning and commissioning ...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Account Executive - Washington DC</td>\n",
       "      <td>US, DC, Washington</td>\n",
       "      <td>THE COMPANY: ESRI – Environmental Systems Rese...</td>\n",
       "      <td>EDUCATION: Bachelor’s or Master’s in GIS, busi...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Bill Review Manager</td>\n",
       "      <td>US, FL, Fort Worth</td>\n",
       "      <td>JOB TITLE: Itemization Review ManagerLOCATION:...</td>\n",
       "      <td>QUALIFICATIONS:RN license in the State of Texa...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       title            location  \\\n",
       "0                           Marketing Intern    US, NY, New York   \n",
       "1  Customer Service - Cloud Video Production      NZ, , Auckland   \n",
       "2    Commissioning Machinery Assistant (CMA)       US, IA, Wever   \n",
       "3          Account Executive - Washington DC  US, DC, Washington   \n",
       "4                        Bill Review Manager  US, FL, Fort Worth   \n",
       "\n",
       "                                         description  \\\n",
       "0  Food52, a fast-growing, James Beard Award-winn...   \n",
       "1  Organised - Focused - Vibrant - Awesome!Do you...   \n",
       "2  Our client, located in Houston, is actively se...   \n",
       "3  THE COMPANY: ESRI – Environmental Systems Rese...   \n",
       "4  JOB TITLE: Itemization Review ManagerLOCATION:...   \n",
       "\n",
       "                                        requirements  telecommuting  \\\n",
       "0  Experience with content management systems a m...              0   \n",
       "1  What we expect from you:Your key responsibilit...              0   \n",
       "2  Implement pre-commissioning and commissioning ...              0   \n",
       "3  EDUCATION: Bachelor’s or Master’s in GIS, busi...              0   \n",
       "4  QUALIFICATIONS:RN license in the State of Texa...              0   \n",
       "\n",
       "   has_company_logo  has_questions  fraudulent  \n",
       "0                 1              0           0  \n",
       "1                 1              0           0  \n",
       "2                 1              0           0  \n",
       "3                 1              0           0  \n",
       "4                 1              1           0  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df1 = pd.read_csv(\"fake_job_postings.csv\")\n",
    "df1.drop(['benefits', 'company_profile', 'employment_type', 'salary_range', 'industry', 'department', 'required_experience', 'required_education', 'job_id', 'function',], axis=1, inplace=True)\n",
    "df2 = pd.read_csv(\"job_train.csv\")\n",
    "merged_df = pd.concat([df1, df2])\n",
    "\n",
    "merged_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "52e329df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "314\n",
      "105\n",
      "9337\n",
      "8921\n",
      "8940\n"
     ]
    }
   ],
   "source": [
    "# print(merged_df.isnull().sum())\n",
    "print(df1.duplicated().sum())\n",
    "print(df2.duplicated().sum())\n",
    "print(merged_df.duplicated().sum())\n",
    "\n",
    "df1_unique = df1.drop_duplicates()\n",
    "df2_unique = df2.drop_duplicates()\n",
    "overlap_count = pd.merge(df1_unique, df2_unique, how='inner').shape[0]\n",
    "print(overlap_count)\n",
    "print(len(df2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94ba35f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42       US\n",
      "173      US\n",
      "230      US\n",
      "368      US\n",
      "392      US\n",
      "         ..\n",
      "17816    US\n",
      "2566     US\n",
      "4370     US\n",
      "5584     GB\n",
      "6858     BH\n",
      "Name: location, Length: 98, dtype: object\n"
     ]
    }
   ],
   "source": [
    "##### Oct 30 Flora #####\n",
    "\n",
    "# Explore \"description\"\n",
    "missing_desc = merged_df[merged_df['description'].isna()]\n",
    "# print(missing_desc[['description', 'fraudulent', 'requirements', 'location']])\n",
    "\n",
    "### Explore \"location\" ###\n",
    "# Extract country and state\n",
    "pattern1 = r'(^[A-Z]{2},\\s*[A-Z0-9]{1,3})'\n",
    "merged_df['country_state'] = merged_df['location'].str.extract(pattern1, expand=False)\n",
    "# Extract country\n",
    "pattern2 = r'(^[A-Z]{2})'\n",
    "merged_df['country'] = merged_df['location'].str.extract(pattern2, expand=False)\n",
    "# Manage Remote jobs\n",
    "merged_df['is_remote'] = merged_df['location'].str.lower().str.contains('remote|work from home', na=False) & merged_df['country_state'].isna()\n",
    "merged_df.loc[merged_df['is_remote'], 'country_state'] = \"Remote\"\n",
    "merged_df.loc[merged_df['is_remote'], 'country'] = \"Remote\"\n",
    "merged_df.drop(columns=[\"is_remote\"], inplace=True)\n",
    "\n",
    "# New column - location mask\n",
    "merged_df[\"has_location\"] = merged_df[\"location\"].notna()\n",
    "# New column - detailed location mask - something beyond just the country code\n",
    "merged_df[\"has_location_details\"] = (merged_df[\"location\"].str.lower().str.strip() == merged_df[\"country\"].str.lower().str.strip()).fillna(False)\n",
    "\n",
    "\n",
    "# unique_outliers = outliers[[\"location\"]].drop_duplicates()\n",
    "# print(\"Outlier rows:\")\n",
    "# print(len(outliers[\"location\"].unique()))\n",
    "# # unique_outliers.to_csv(\"location_outliers\")\n",
    "# print(unique_outliers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d0ec957",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0     True\n",
      "1     True\n",
      "2     True\n",
      "3     True\n",
      "4     True\n",
      "5    False\n",
      "6     True\n",
      "7     True\n",
      "8     True\n",
      "9     True\n",
      "Name: has_requirements, dtype: bool\n",
      "0    Food52, a fast-growing, James Beard Award-winn...\n",
      "1    Organised - Focused - Vibrant - Awesome!Do you...\n",
      "2    Our client, located in Houston, is actively se...\n",
      "3    THE COMPANY: ESRI – Environmental Systems Rese...\n",
      "4    JOB TITLE: Itemization Review ManagerLOCATION:...\n",
      "Name: description_and_requirements, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Make new feature that has binary value for whether requirements was missing or not\n",
    "merged_df[\"has_requirements\"] = merged_df[\"requirements\"].notna()\n",
    "print(merged_df[\"has_requirements\"].head(10))\n",
    "\n",
    "# Merge description with requirements so they are in one new feature - called description_and_requirements\n",
    "merged_df[\"description_and_requirements\"] = merged_df[\"description\"].fillna(\"\") + merged_df[\"requirements\"].fillna(\"\")\n",
    "print(merged_df[\"description_and_requirements\"].head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d36aec7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
